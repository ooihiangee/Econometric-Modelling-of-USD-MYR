{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Timeseries kinds and applications**\n",
    "___\n",
    "- data that changes over time\n",
    "    - e.g., atmospheric changes, demographic information, financial data, voice wave forms\n",
    "    - datapoints and timestamps for each data point\n",
    "- in machine learning, changes over time shows useful patterns in machine learning\n",
    "- a machine learning pipeline\n",
    "    - feature extraction\n",
    "    - model fitting\n",
    "    - prediction and validation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Plotting a time series (I)\n",
    "\n",
    "#In this exercise, you'll practice plotting the values of two time\n",
    "#series without the time component.\n",
    "\n",
    "#Two DataFrames, data and data2 are available in your workspace.\n",
    "\n",
    "#Unless otherwise noted, assume that all required packages are loaded\n",
    "#with their common aliases throughout this course.\n",
    "\n",
    "#Note: This course assumes some familiarity with time series data,\n",
    "#as well as how to use them in data analytics pipelines. For an\n",
    "#introduction to time series, we recommend the Introduction to Time\n",
    "#Series Analysis in Python and Visualizing Time Series Data with Python\n",
    "#courses.\n",
    "\n",
    "# Print the first 5 rows of data\n",
    "#print(data.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    symbol  data_values\n",
    "#    0        214.009998\n",
    "#    1        214.379993\n",
    "#    2        210.969995\n",
    "#    3        210.580000\n",
    "#    4        211.980005\n",
    "#################################################\n",
    "\n",
    "# Print the first 5 rows of data2\n",
    "#print(data2.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       data_values\n",
    "#    0    -0.006928\n",
    "#    1    -0.007929\n",
    "#    2    -0.008900\n",
    "#    3    -0.009815\n",
    "#    4    -0.010653\n",
    "#################################################\n",
    "\n",
    "# Plot the time series in each dataset\n",
    "#fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
    "#data.iloc[:1000].plot(y=\"data_values\", ax=axs[0])\n",
    "#data2.iloc[:1000].plot(y=\"data_values\", ax=axs[1])\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.1.svg](_images/15.1.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Plotting a time series (II)\n",
    "\n",
    "#You'll now plot both the datasets again, but with the included time\n",
    "#stamps for each (stored in the column called \"time\"). Let's see if\n",
    "#this gives you some more context for understanding each time series\n",
    "#data.\n",
    "\n",
    "# Plot the time series in each dataset\n",
    "#fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
    "#data.iloc[:1000].plot(x=\"time\", y=\"data_values\", ax=axs[0])\n",
    "#data2.iloc[:1000].plot(x=\"time\", y=\"data_values\", ax=axs[1])\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.2.svg](_images/15.2.svg)\n",
    "As you can now see, each time series has a very different sampling\n",
    "frequency (the amount of time between samples). The first is daily\n",
    "stock market data, and the second is an audio waveform."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Machine learning basics**\n",
    "___\n",
    "- always begin by looking at your data\n",
    "- scikit-learn data needs to be 2 dimensional\n",
    "    - (samples, features)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fitting a simple model: classification\n",
    "\n",
    "#In this exercise, you'll use the iris dataset (representing petal\n",
    "#characteristics of a number of flowers) to practice using the\n",
    "#scikit-learn API to fit a classification model. You can see a sample\n",
    "#plot of the data below.\n",
    "\n",
    "#Note: This course assumes some familiarity with Machine Learning\n",
    "#and scikit-learn. For an introduction to scikit-learn, we recommend\n",
    "#the Supervised Learning with Scikit-Learn and Preprocessing for\n",
    "#Machine Learning in Python courses."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.3.svg](_images/15.3.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the first 5 rows for inspection\n",
    "#print(data.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#        sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
    "#    50                7.0               3.2                4.7               1.4\n",
    "#    51                6.4               3.2                4.5               1.5\n",
    "#    52                6.9               3.1                4.9               1.5\n",
    "#    53                5.5               2.3                4.0               1.3\n",
    "#    54                6.5               2.8                4.6               1.5\n",
    "#\n",
    "#        target\n",
    "#    50       1\n",
    "#    51       1\n",
    "#    52       1\n",
    "#    53       1\n",
    "#    54       1\n",
    "#################################################\n",
    "\n",
    "#from sklearn.svm import LinearSVC\n",
    "\n",
    "# Construct data for the model\n",
    "#X = data[['petal length (cm)', 'petal width (cm)']]\n",
    "#y = data[['target']]\n",
    "\n",
    "# Fit the model\n",
    "#model = LinearSVC()\n",
    "#model.fit(X, y)\n",
    "\n",
    "#################################################\n",
    "#You've successfully fit a classifier to predict flower type!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Predicting using a classification model\n",
    "\n",
    "#Now that you have fit your classifier, let's use it to predict the\n",
    "#type of flower (or class) for some newly-collected flowers.\n",
    "\n",
    "#Information about petal width and length for several new flowers is\n",
    "#stored in the variable targets. Using the classifier you fit, you'll\n",
    "#predict the type of each flower.\n",
    "\n",
    "# Create input array\n",
    "#X_predict = targets[['petal length (cm)', 'petal width (cm)']]\n",
    "\n",
    "# Predict with the model\n",
    "#predictions = model.predict(X_predict)\n",
    "#print(predictions)\n",
    "\n",
    "# Visualize predictions and actual values\n",
    "#plt.scatter(X_predict['petal length (cm)'], X_predict['petal width (cm)'],\n",
    "#            c=predictions, cmap=plt.cm.coolwarm)\n",
    "#plt.title(\"Predicted class values\")\n",
    "#plt.show()\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [2 2 2 1 1 2 2 2 2 1 2 1 1 2 1 1 2 1 2 2]\n",
    "#################################################\n",
    "#Note that the output of your predictions are all integers,\n",
    "#representing that datapoint's predicted class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.4.svg](_images/15.4.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fitting a simple model: regression\n",
    "\n",
    "#In this exercise, you'll practice fitting a regression model using\n",
    "#data from the Boston housing market. A DataFrame called boston is\n",
    "#available in your workspace. It contains many variables of data\n",
    "#(stored as columns). Can you find a relationship between the\n",
    "#following two variables?\n",
    "\n",
    "#\"AGE\": proportion of owner-occupied units built prior to 1940\n",
    "#\"RM\" : average number of rooms per dwelling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.5.svg](_images/15.5.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#from sklearn import linear_model\n",
    "\n",
    "# Prepare input and output DataFrames\n",
    "#X = boston[['AGE']]\n",
    "#y = boston[['RM']]\n",
    "\n",
    "# Fit the model\n",
    "#model = linear_model.LinearRegression()\n",
    "#model.fit(X, y)\n",
    "\n",
    "#################################################\n",
    "# In regression, the output of your model is a continuous array of\n",
    "#numbers, not class identity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Predicting using a regression model\n",
    "\n",
    "#Now that you've fit a model with the Boston housing data, lets see\n",
    "#what predictions it generates on some new data. You can investigate\n",
    "#the underlying relationship that the model has found between inputs\n",
    "#and outputs by feeding in a range of numbers as inputs and seeing\n",
    "#what the model predicts for each input.\n",
    "\n",
    "#A 1-D array new_inputs consisting of 100 \"new\" values for \"AGE\"\n",
    "#(proportion of owner-occupied units built prior to 1940) is\n",
    "#available in your workspace along with the model you fit in the\n",
    "#previous exercise.\n",
    "\n",
    "# Generate predictions with the model using those inputs\n",
    "#predictions = model.predict(new_inputs.reshape(-1, 1))\n",
    "\n",
    "# Visualize the inputs and predicted values\n",
    "#plt.scatter(new_inputs, predictions, color='r', s=3)\n",
    "#plt.xlabel('inputs')\n",
    "#plt.ylabel('predictions')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.6.svg](_images/15.6.svg)\n",
    "Here the red line shows the relationship that your model found. As\n",
    "the proportion of pre-1940s houses gets larger, the average number\n",
    "of rooms gets slightly lower."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Machine learning and time series data**\n",
    "___\n",
    "- using audio data of heart sounds to detect who has a heart condition\n",
    "- using new york stock exchange data to detect patterns in historical records that allow us to predict the value of companies in the future\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Inspecting the classification data\n",
    "\n",
    "#In these final exercises of this chapter, you'll explore the two\n",
    "#datasets you'll use in this course.\n",
    "\n",
    "#The first is a collection of heartbeat sounds. Hearts normally have\n",
    "#a predictable sound pattern as they beat, but some disorders can\n",
    "#cause the heart to beat abnormally. This dataset contains a training\n",
    "#set with labels for each type of heartbeat, and a testing set with no\n",
    "#labels. You'll use the testing set to validate your models.\n",
    "\n",
    "#As you have labeled data, this dataset is ideal for classification.\n",
    "#In fact, it was originally offered as a part of a public Kaggle\n",
    "#competition. https://www.kaggle.com/kinguistics/heartbeat-sounds\n",
    "\n",
    "#import librosa as lr\n",
    "#from glob import glob\n",
    "\n",
    "# List all the wav files in the folder\n",
    "#audio_files = glob(data_dir + '/*.wav')\n",
    "\n",
    "# Read in the first audio file, create the time array\n",
    "#audio, sfreq = lr.load(audio_files[0])\n",
    "#time = np.arange(0, len(audio)) / sfreq\n",
    "\n",
    "# Plot audio over time\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(time, audio)\n",
    "#ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.7.svg](_images/15.7.svg)\n",
    "There are several seconds of heartbeat sounds in here, though note\n",
    "that most of this time is silence. A common procedure in machine\n",
    "learning is to separate the datapoints with lots of stuff happening\n",
    "from the ones that don't."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Inspecting the regression data\n",
    "\n",
    "#The next dataset contains information about company market value\n",
    "#over several years of time. This is one of the most popular kind\n",
    "#of time series data used for regression. If you can model the value\n",
    "#of a company as it changes over time, you can make predictions about\n",
    "#where that company will be in the future. This dataset was also\n",
    "#originally provided as part of a public Kaggle competition.\n",
    "#https://www.kaggle.com/dgawlik/nyse\n",
    "\n",
    "#In this exercise, you'll plot the time series for a number of\n",
    "#companies to get an understanding of how they are (or aren't)\n",
    "#related to one another.\n",
    "\n",
    "# Read in the data\n",
    "#data = pd.read_csv('prices.csv', index_col=0)\n",
    "\n",
    "# Convert the index of the DataFrame to datetime\n",
    "#data.index = pd.to_datetime(data.index)\n",
    "#print(data.head())\n",
    "\n",
    "# Loop through each column, plot its values over time\n",
    "#fig, ax = plt.subplots()\n",
    "#for column in data.columns:\n",
    "#    data[column].plot(ax=ax, label=column)\n",
    "#ax.legend()\n",
    "#plt.show()\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                      AAPL  FB       NFLX          V        XOM\n",
    "#    time\n",
    "#    2010-01-04  214.009998 NaN  53.479999  88.139999  69.150002\n",
    "#    2010-01-05  214.379993 NaN  51.510001  87.129997  69.419998\n",
    "#    2010-01-06  210.969995 NaN  53.319999  85.959999  70.019997\n",
    "#    2010-01-07  210.580000 NaN  52.400001  86.760002  69.800003\n",
    "#    2010-01-08  211.980005 NaN  53.300002  87.000000  69.519997\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.8.svg](_images/15.8.svg)\n",
    "Note that each company's value is sometimes correlated with others,\n",
    "and sometimes not. Also note there are a lot of 'jumps' in there -\n",
    "what effect do you think these jumps would have on a predictive model?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Classifying a time series**\n",
    "___\n",
    "- always visualize raw data before fitting models\n",
    "- start with summary statistics\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Many repetitions of sounds\n",
    "\n",
    "#In this exercise, you'll start with perhaps the simplest\n",
    "#classification technique: averaging across dimensions of a dataset\n",
    "#and visually inspecting the result.\n",
    "\n",
    "#You'll use the heartbeat data described in the last chapter. Some\n",
    "#recordings are normal heartbeat activity, while others are abnormal\n",
    "#activity. Let's see if you can spot the difference.\n",
    "\n",
    "#Two DataFrames, normal and abnormal, each with the shape of\n",
    "#(n_times_points, n_audio_files) containing the audio for several\n",
    "#heartbeats are available in your workspace. Also, the sampling\n",
    "#frequency is loaded into a variable called sfreq. A convenience\n",
    "#plotting function show_plot_and_make_titles() is also available in\n",
    "#your workspace.\n",
    "\n",
    "#fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
    "\n",
    "# Calculate the time array\n",
    "#time = np.arange(normal.shape[0]) / sfreq\n",
    "\n",
    "# Stack the normal/abnormal audio so you can loop and plot\n",
    "#stacked_audio = np.hstack([normal, abnormal]).T\n",
    "\n",
    "# Loop through each audio file / ax object and plot\n",
    "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
    "#for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
    "#    ax.plot(time, iaudio)\n",
    "#show_plot_and_make_titles()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.9.svg](_images/15.9.svg)\n",
    "As you can see there is a lot of variability in the raw data, let's\n",
    "see if you can average out some of that noise to notice a difference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Invariance in time\n",
    "\n",
    "#While you should always start by visualizing your raw data, this is\n",
    "#often uninformative when it comes to discriminating between two\n",
    "#classes of data points. Data is usually noisy or exhibits complex\n",
    "#patterns that aren't discoverable by the naked eye.\n",
    "\n",
    "#Another common technique to find simple differences between two\n",
    "#sets of data is to average across multiple instances of the same\n",
    "#class. This may remove noise and reveal underlying patterns (or,\n",
    "#it may not).\n",
    "\n",
    "#In this exercise, you'll average across many instances of each\n",
    "#class of heartbeat sound.\n",
    "\n",
    "#The two DataFrames (normal and abnormal) and the time array (time)\n",
    "#from the previous exercise are available in your workspace.\n",
    "\n",
    "# Average across the audio files of each DataFrame\n",
    "#mean_normal = np.mean(normal, axis=1)\n",
    "#mean_abnormal = np.mean(abnormal, axis=1)\n",
    "\n",
    "# Plot each average over time\n",
    "#fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "#ax1.plot(time, mean_normal)\n",
    "#ax1.set(title=\"Normal Data\")\n",
    "#ax2.plot(time, mean_abnormal)\n",
    "#ax2.set(title=\"Abnormal Data\")\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.10.svg](_images/15.10.svg)\n",
    "Do you see a noticeable difference between the two? Maybe, but it's\n",
    "quite noisy. Let's see how you can dig into the data a bit further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Build a classification model\n",
    "\n",
    "#While eye-balling differences is a useful way to gain an intuition\n",
    "#for the data, let's see if you can operationalize things with a\n",
    "#model. In this exercise, you will use each repetition as a\n",
    "#datapoint, and each moment in time as a feature to fit a classifier\n",
    "#that attempts to predict abnormal vs. normal heartbeats using only\n",
    "#the raw data.\n",
    "\n",
    "#We've split the two DataFrames (normal and abnormal) into X_train,\n",
    "#X_test, y_train, and y_test.\n",
    "\n",
    "#from sklearn.svm import LinearSVC\n",
    "\n",
    "# Initialize and fit the model\n",
    "#model = LinearSVC()\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and score them manually\n",
    "#predictions = model.predict(X_test)\n",
    "#print(sum(predictions == y_test.squeeze()) / len(y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.555555555556\n",
    "#################################################\n",
    "#Note that your predictions didn't do so well. That's because the\n",
    "#features you're using as inputs to the model (raw data) aren't very\n",
    "#good at differentiating classes. Next, you'll explore how to calculate\n",
    "#some more complex features that may improve the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Improving features for classification**\n",
    "___\n",
    "- The auditory envelope\n",
    "    - smooth the data to calculate the auditory envelope\n",
    "    - related to the amount of audio energy present at each moment in time\n",
    "- smoothing over time\n",
    "    - instead of averaging over time, we do a local average\n",
    "    - this is called smoothing your timeseries\n",
    "    - it removes short-term noise, while retaining the general pattern\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Calculating the envelope of sound\n",
    "#One of the ways you can improve the features available to your\n",
    "#model is to remove some of the noise present in the data. In audio\n",
    "#data, a common way to do this is to smooth the data and then rectify\n",
    "#it so that the total amount of sound energy over time is more\n",
    "#distinguishable. You'll do this in the current exercise.\n",
    "\n",
    "#A heartbeat file is available in the variable audio.\n",
    "\n",
    "# Plot the raw data first\n",
    "#audio.plot(figsize=(10, 5))\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.11.svg](_images/15.11.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rectify the audio signal\n",
    "#audio_rectified = audio.apply(np.abs)\n",
    "\n",
    "# Plot the result\n",
    "#audio_rectified.plot(figsize=(10, 5))\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.12.svg](_images/15.12.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Smooth by applying a rolling mean\n",
    "#audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
    "\n",
    "# Plot the result\n",
    "#audio_rectified_smooth.plot(figsize=(10, 5))\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.13.svg](_images/15.13.svg)\n",
    "By calculating the envelope of each sound and smoothing it, you've\n",
    "eliminated much of the noise and have a cleaner signal to tell you\n",
    "when a heartbeat is happening."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Calculating features from the envelope\n",
    "\n",
    "#Now that you've removed some of the noisier fluctuations in the\n",
    "#audio, let's see if this improves your ability to classify.\n",
    "\n",
    "#audio_rectified_smooth from the previous exercise is available in\n",
    "#your workspace.\n",
    "\n",
    "# Calculate stats\n",
    "#means = np.mean(audio_rectified_smooth, axis=0)\n",
    "#stds = np.std(audio_rectified_smooth, axis=0)\n",
    "#maxs = np.max(audio_rectified_smooth, axis=0)\n",
    "\n",
    "# Create the X and y arrays\n",
    "#X = np.column_stack([means, stds, maxs])\n",
    "#y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#percent_score = cross_val_score(model, X, y, cv=5)\n",
    "#print(np.mean(percent_score))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.716666666667\n",
    "#################################################\n",
    "#This model is both simpler (only 3 features) and more understandable\n",
    "#(features are simple summary statistics of the data)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Derivative features: The tempogram\n",
    "\n",
    "#One benefit of cleaning up your data is that it lets you compute\n",
    "#more sophisticated features. For example, the envelope calculation\n",
    "#you performed is a common technique in computing tempo and rhythm\n",
    "#features. In this exercise, you'll use librosa to compute some\n",
    "#tempo and rhythm features for heartbeat data, and fit a model once\n",
    "#more.\n",
    "\n",
    "#Note that librosa functions tend to only operate on numpy arrays\n",
    "#instead of DataFrames, so we'll access our Pandas data as a Numpy\n",
    "#array with the .values attribute.\n",
    "\n",
    "# Calculate the tempo of the sounds\n",
    "#tempos = []\n",
    "#for col, i_audio in audio.items():\n",
    "#    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
    "\n",
    "# Convert the list to an array so you can manipulate it more easily\n",
    "#tempos = np.array(tempos)\n",
    "\n",
    "# Calculate statistics of each tempo\n",
    "#tempos_mean = tempos.mean(axis=-1)\n",
    "#tempos_std = tempos.std(axis=-1)\n",
    "#tempos_max = tempos.max(axis=-1)\n",
    "\n",
    "# Create the X and y arrays\n",
    "#X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
    "#y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "#percent_score = cross_val_score(model, X, y, cv=5)\n",
    "#print(np.mean(percent_score))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.533333333333\n",
    "#################################################\n",
    "#Note that your predictive power may not have gone up (because this\n",
    "#dataset is quite small), but you now have a more rich feature\n",
    "#representation of audio that your model can use!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The spectrogram**\n",
    "___\n",
    "- fourier transform\n",
    "    - timeseries data can be described as a combination of quickly-changing and slowly-changing things\n",
    "    - at each moment in time, we can describe the relative presence of fast- and slow-moving components\n",
    "    - this converts a single timeseries into an array that describes the timeseries as a combination of oscillations\n",
    "- short time (st) fft is squared = spectrogram\n",
    "- spectral feature engineering\n",
    "    - each timeseries has a different spectral pattern\n",
    "    - we can calculate these spectral patterns by analyzing the spectrogram to describe where most of the energy is at each moment in time\n",
    "        - **spectral bandwidth**\n",
    "        - **spectral centroids**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Spectrograms of heartbeat audio\n",
    "\n",
    "#Spectral engineering is one of the most common techniques in\n",
    "#machine learning for time series data. The first step in this\n",
    "#process is to calculate a spectrogram of sound. This describes what\n",
    "#spectral content (e.g., low and high pitches) are present in the\n",
    "#sound over time. In this exercise, you'll calculate a spectrogram\n",
    "#of a heartbeat audio file.\n",
    "\n",
    "#We've loaded a single heartbeat sound in the variable audio.\n",
    "\n",
    "# Import the functions you'll use for the STFT\n",
    "#from librosa.core import stft\n",
    "\n",
    "# Prepare the STFT\n",
    "#HOP_LENGTH = 2**4\n",
    "#spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
    "\n",
    "#from librosa.core import amplitude_to_db\n",
    "#from librosa.display import specshow\n",
    "\n",
    "# Convert into decibels\n",
    "#spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Compare the raw audio to the spectrogram of the audio\n",
    "#fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "#axs[0].plot(time, audio)\n",
    "#specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.14.svg](_images/15.14.svg)\n",
    "Do you notice that the heartbeats come in pairs, as seen by the\n",
    "vertical lines in the spectrogram?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Engineering spectral features\n",
    "\n",
    "#As you can probably tell, there is a lot more information in a\n",
    "#spectrogram compared to a raw audio file. By computing the spectral\n",
    "#features, you have a much better idea of what's going on. As such,\n",
    "#there are all kinds of spectral features that you can compute using\n",
    "#the spectrogram as a base. In this exercise, you'll look at a few\n",
    "#of these features.\n",
    "\n",
    "#The spectogram spec from the previous exercise is available in your\n",
    "#workspace.\n",
    "\n",
    "#import librosa as lr\n",
    "\n",
    "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "#bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "#centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
    "\n",
    "#from librosa.core import amplitude_to_db\n",
    "#from librosa.display import specshow\n",
    "\n",
    "# Convert spectrogram to decibels for visualization\n",
    "#spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Display these features on top of the spectrogram\n",
    "#fig, ax = plt.subplots(figsize=(10, 5))\n",
    "#ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
    "#ax.plot(times_spec, centroids)\n",
    "#ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
    "#ax.set(ylim=[None, 6000])\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.15.svg](_images/15.15.svg)\n",
    "As you can see, the spectral centroid and bandwidth characterize the\n",
    "spectral content in each sound over time. They give us a summary of\n",
    "the spectral content that we can use in a classifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Combining many features in a classifier\n",
    "\n",
    "#You've spent this lesson engineering many features from the audio\n",
    "#data - some contain information about how the audio changes in time,\n",
    "#others contain information about the spectral content that is\n",
    "#present.\n",
    "\n",
    "#The beauty of machine learning is that it can handle all of these\n",
    "#features at the same time. If there is different information present\n",
    "#in each feature, it should improve the classifier's ability to\n",
    "#distinguish the types of audio. Note that this often requires more\n",
    "#advanced techniques such as regularization, which we'll cover in\n",
    "#the next chapter.\n",
    "\n",
    "#For the final exercise in the chapter, we've loaded many of the\n",
    "#features that you calculated before. Combine all of them into an\n",
    "#array that can be fed into the classifier, and see how it does.\n",
    "\n",
    "# Loop through each spectrogram\n",
    "#bandwidths = []\n",
    "#centroids = []\n",
    "\n",
    "#for spec in spectrograms:\n",
    "    # Calculate the mean spectral bandwidth\n",
    "#    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
    "    # Calculate the mean spectral centroid\n",
    "#    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
    "    # Collect the values\n",
    "#    bandwidths.append(this_mean_bandwidth)\n",
    "#    centroids.append(this_mean_centroid)\n",
    "\n",
    "# Create the X and y arrays\n",
    "#X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
    "#y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "#percent_score = cross_val_score(model, X, y, cv=5)\n",
    "#print(np.mean(percent_score))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.483333333333\n",
    "#################################################\n",
    "#You calculated many different features of the audio, and combined\n",
    "#each of them under the assumption that they provide independent\n",
    "#information that can be used in classification. You may have noticed\n",
    "#that the accuracy of your models varied a lot when using different\n",
    "#set of features. This chapter was focused on creating new \"features\"\n",
    "#from raw data and not obtaining the best accuracy. To improve the\n",
    "#accuracy, you want to find the right features that provide relevant\n",
    "#information and also build models on much larger data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Predicting data over time**\n",
    "___\n",
    "- correlation between variables often changes over time\n",
    "    - two timeseries that seem correlated at one moment may not remain so over time\n",
    "- Coefficient of Determination (R squared)\n",
    "    - 1 - (error of the model / variance of test data\n",
    "    - \"model accounts for % of variance of the model\"\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Introducing the dataset\n",
    "\n",
    "#As mentioned in the video, you'll deal with stock market prices that\n",
    "#fluctuate over time. In this exercise you've got historical prices\n",
    "#from two tech companies (Ebay and Yahoo) in the DataFrame prices.\n",
    "#You'll visualize the raw data for the two companies, then generate\n",
    "#a scatter plot showing how the values for each company compare with\n",
    "#one another. Finally, you'll add in a \"time\" dimension to your\n",
    "#scatter plot so you can see how this relationship changes over time.\n",
    "\n",
    "#The data has been loaded into a DataFrame called prices.\n",
    "\n",
    "# Plot the raw values over time\n",
    "#prices.plot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.16.svg](_images/15.16.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scatterplot with one company per axis\n",
    "#prices.plot.scatter(\"EBAY\", \"YHOO\")\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.17.svg](_images/15.17.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scatterplot with color relating to time\n",
    "#prices.plot.scatter('EBAY', 'YHOO', c=prices.index,\n",
    "#                    cmap=plt.cm.viridis, colorbar=False)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.18.svg](_images/15.18.svg)\n",
    "As you can see, these two time series seem somewhat related to each\n",
    "other, though its a complex relationship that changes over time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fitting a simple regression model\n",
    "\n",
    "#Now we'll look at a larger number of companies. Recall that we have\n",
    "#historical price values for many companies. Let's use data from\n",
    "#several companies to predict the value of a test company. You'll\n",
    "#attempt to predict the value of the Apple stock price using the\n",
    "#values of NVidia, Ebay, and Yahoo. Each of these is stored as a\n",
    "#column in the all_prices DataFrame. Below is a mapping from company\n",
    "#name to column name:\n",
    "\n",
    "#ebay: \"EBAY\"\n",
    "#nvidia: \"NVDA\"\n",
    "#yahoo: \"YHOO\"\n",
    "#apple: \"AAPL\"\n",
    "#We'll use these columns to define the input/output arrays in our\n",
    "#model.\n",
    "\n",
    "#from sklearn.linear_model import Ridge\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use stock symbols to extract training data\n",
    "#X = all_prices[['EBAY', 'NVDA', 'YHOO']]\n",
    "#y = all_prices[['AAPL']]\n",
    "\n",
    "# Fit and score the model with cross-validation\n",
    "#scores = cross_val_score(Ridge(), X, y, cv=3)\n",
    "#print(scores)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [-6.09050633 -0.3179172  -3.72957284]\n",
    "#################################################\n",
    "#As you can see, fitting a model with raw data doesn't give great results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing predicted values\n",
    "\n",
    "#When dealing with time series data, it's useful to visualize model\n",
    "#predictions on top of the \"actual\" values that are used to test the\n",
    "#model.\n",
    "\n",
    "#In this exercise, after splitting the data (stored in the variables\n",
    "#X and y) into training and test sets, you'll build a model and then\n",
    "#visualize the model's predictions on top of the testing data in order\n",
    "#to estimate the model's performance.\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import r2_score\n",
    "\n",
    "# Split our data into training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                    train_size=.8, shuffle=False, random_state=1)\n",
    "\n",
    "# Fit our model and generate predictions\n",
    "#model = Ridge()\n",
    "#model.fit(X_train, y_train)\n",
    "#predictions = model.predict(X_test)\n",
    "#score = r2_score(y_test, predictions)\n",
    "#print(score)\n",
    "\n",
    "# Visualize our predictions along with the \"true\" values, and print the score\n",
    "#fig, ax = plt.subplots(figsize=(15, 5))\n",
    "#ax.plot(y_test, color='k', lw=3)\n",
    "#ax.plot(predictions, color='r', lw=2)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.19.svg](_images/15.19.svg)\n",
    "Now you have an explanation for your poor score. The predictions\n",
    "clearly deviate from the true time series values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Advanced time series prediction**\n",
    "___\n",
    "- cleaning messy data\n",
    "    - interpolating missing values\n",
    "    - transforming data to standardize variance\n",
    "    - finding outliers (> 3 sd) in data\n",
    "        - replace outliers with threeshold/median\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing messy data\n",
    "\n",
    "#Let's take a look at a new dataset - this one is a bit less-clean\n",
    "#than what you've seen before.\n",
    "\n",
    "#As always, you'll first start by visualizing the raw data. Take a\n",
    "#close look and try to find datapoints that could be problematic for\n",
    "#fitting models.\n",
    "\n",
    "#The data has been loaded into a DataFrame called prices.\n",
    "\n",
    "# Visualize the dataset\n",
    "#prices.plot(legend=False)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "# Count the missing values of each time series\n",
    "#missing_values = prices.isna().sum()\n",
    "#print(missing_values)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    symbol\n",
    "#    EBAY    273\n",
    "#    NVDA    502\n",
    "#    YHOO    232\n",
    "#    dtype: int64\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.20.svg](_images/15.20.svg)\n",
    "In the plot, you can see there are clearly missing chunks of time\n",
    "in your data. There also seem to be a few 'jumps' in the data. How\n",
    "can you deal with this?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Imputing missing values\n",
    "\n",
    "#When you have missing data points, how can you fill them in?\n",
    "\n",
    "#In this exercise, you'll practice using different interpolation\n",
    "#methods to fill in some missing values, visualizing the result each\n",
    "#time. But first, you will create the function (interpolate_and_plot())\n",
    "#you'll use to interpolate missing data points and plot them.\n",
    "\n",
    "#A single time series has been loaded into a DataFrame called prices.\n",
    "\n",
    "# Create a function we'll use to interpolate and plot\n",
    "#def interpolate_and_plot(prices, interpolation):\n",
    "\n",
    "    # Create a boolean mask for missing values\n",
    "#    missing_values = prices.isna()\n",
    "\n",
    "    # Interpolate the missing values\n",
    "#    prices_interp = prices.interpolate(interpolation)\n",
    "\n",
    "    # Plot the results, highlighting the interpolated values in black\n",
    "#    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "#    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
    "\n",
    "    # Now plot the interpolated values on top in red\n",
    "#    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
    "#    plt.show()\n",
    "\n",
    "# Interpolate using the latest non-missing value\n",
    "#interpolation_type = 'zero'\n",
    "#interpolate_and_plot(prices, interpolation_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.21.svg](_images/15.21.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Interpolate linearly\n",
    "#interpolation_type = 'linear'\n",
    "#interpolate_and_plot(prices, interpolation_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.22.svg](_images/15.22.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Interpolate with a quadratic function\n",
    "#interpolation_type = 'quadratic'\n",
    "#interpolate_and_plot(prices, interpolation_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.23.svg](_images/15.23.svg)\n",
    "When you interpolate, the pre-existing data is used to infer the\n",
    "values of missing data. As you can see, the method you use for this\n",
    "has a big effect on the outcome."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Transforming raw data\n",
    "\n",
    "#In the last chapter, you calculated the rolling mean. In this\n",
    "#exercise, you will define a function that calculates the percent\n",
    "#change of the latest data point from the mean of a window of\n",
    "#previous data points. This function will help you calculate the\n",
    "#percent change over a rolling window.\n",
    "\n",
    "#This is a more stable kind of time series that is often useful in\n",
    "#machine learning.\n",
    "\n",
    "# Your custom function\n",
    "#def percent_change(series):\n",
    "    # Collect all *but* the last value of this window, then the final value\n",
    "#    previous_values = series[:-1]\n",
    "#    last_value = series[-1]\n",
    "\n",
    "    # Calculate the % difference between the last value and the mean of earlier values\n",
    "#    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
    "#    return percent_change\n",
    "\n",
    "# Apply your custom function and plot\n",
    "#prices_perc = prices.rolling(20).apply(percent_change)\n",
    "#prices_perc.loc[\"2014\":\"2015\"].plot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.24.svg](_images/15.24.svg)\n",
    "You've converted the data so it's easier to compare one time point\n",
    "to another. This is a cleaner representation of the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Handling outliers\n",
    "\n",
    "#In this exercise, you'll handle outliers - data points that are so\n",
    "#different from the rest of your data, that you treat them differently\n",
    "#from other \"normal-looking\" data points. You'll use the output from\n",
    "#the previous exercise (percent change over time) to detect the outliers.\n",
    "#First you will write a function that replaces outlier data points with\n",
    "#the median value from the entire time series.\n",
    "\n",
    "#def replace_outliers(series):\n",
    "    # Calculate the absolute difference of each timepoint from the series mean\n",
    "#    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
    "\n",
    "    # Calculate a mask for the differences that are > 3 standard deviations from the mean\n",
    "#    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
    "\n",
    "    # Replace these values with the median accross the data\n",
    "#    series[this_mask] = np.nanmedian(series)\n",
    "#    return series\n",
    "\n",
    "# Apply your preprocessing function to the timeseries and plot the results\n",
    "#prices_perc = prices_perc.apply(replace_outliers)\n",
    "#prices_perc.loc[\"2014\":\"2015\"].plot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.25.svg](_images/15.25.svg)\n",
    "Since you've converted the data to % change over time, it was easier\n",
    "to spot and correct the outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating features over time**\n",
    "___\n",
    "- using .aggregate for feature extraction\n",
    "- using .partial() functions in Python\n",
    "- percentiles summarize your data\n",
    "    - percentiles are a useful way to get more fine-grained summaries of your data (as opposed to using np.mean)\n",
    "- calculating \"date-based\" features\n",
    "    - using Pandas .index\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Engineering multiple rolling features at once\n",
    "\n",
    "#Now that you've practiced some simple feature engineering, let's\n",
    "#move on to something more complex. You'll calculate a collection of\n",
    "#features for your time series data and visualize what they look like\n",
    "#over time. This process resembles how many other time series models\n",
    "#operate.\n",
    "\n",
    "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
    "#prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
    "\n",
    "# Define the features you'll calculate for each window\n",
    "#features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
    "\n",
    "# Calculate these features for your rolling window object\n",
    "#features = prices_perc_rolling.aggregate(features_to_calculate)\n",
    "\n",
    "# Plot the results\n",
    "#ax = features.loc[:\"2011-01\"].plot()\n",
    "#prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
    "#ax.legend(loc=(1.01, .6))\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.26.svg](_images/15.26.svg)\n",
    "In the next exercise, you will calculate the percentiles."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Percentiles and partial functions\n",
    "\n",
    "#In this exercise, you'll practice how to pre-choose arguments of a\n",
    "#function so that you can pre-configure how it runs. You'll use this\n",
    "#to calculate several percentiles of your data using the same\n",
    "#percentile() function in numpy.\n",
    "\n",
    "# Import partial from functools\n",
    "#from functools import partial\n",
    "#percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
    "\n",
    "# Use a list comprehension to create a partial function for each quantile\n",
    "#percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
    "\n",
    "# Calculate each of these quantiles on the data using a rolling window\n",
    "#prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
    "#features_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n",
    "\n",
    "# Plot a subset of the result\n",
    "#ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
    "#ax.legend(percentiles, loc=(1.01, .5))\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.27.svg](_images/15.27.svg)\n",
    "In the next exercise, you will extract the date components of the timestamps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using \"date\" information\n",
    "\n",
    "#It's easy to think of timestamps as pure numbers, but don't forget\n",
    "#they generally correspond to things that happen in the real world.\n",
    "#That means there's often extra information encoded in the data such\n",
    "#as \"is it a weekday?\" or \"is it a holiday?\". This information is\n",
    "#often useful in predicting timeseries data.\n",
    "\n",
    "#In this exercise, you'll extract these date/time based features. A\n",
    "#single time series has been loaded in a variable called prices.\n",
    "\n",
    "# Extract date features from the data, add them as columns\n",
    "#prices_perc['day_of_week'] = prices_perc.index.dayofweek\n",
    "#prices_perc['week_of_year'] = prices_perc.index.weekofyear\n",
    "#prices_perc['month_of_year'] = prices_perc.index.month\n",
    "\n",
    "# Print prices_perc\n",
    "#print(prices_perc)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                    EBAY  day_of_week  week_of_year  month_of_year\n",
    "#    date\n",
    "#    2014-01-02  0.017938            3             1              1\n",
    "#    2014-01-03  0.002268            4             1              1\n",
    "#    2014-01-06 -0.027365            0             2              1\n",
    "#    2014-01-07 -0.006665            1             2              1\n",
    "#    2014-01-08 -0.017206            2             2              1\n",
    "#    2014-01-09 -0.023270            3             2              1\n",
    "#    2014-01-10 -0.022257            4             2              1\n",
    "#\n",
    "#    ...              ...          ...           ...            ...\n",
    "#\n",
    "#    [504 rows x 4 columns]\n",
    "#################################################\n",
    "#This concludes the third chapter. In the next chapter, you will\n",
    "#learn advanced techniques to validate and inspect your time series\n",
    "#models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating features from the past**\n",
    "___\n",
    "- The past is useful\n",
    "    - timeseries data almost always have information that is shared between timepoints\n",
    "    - information in the past can help predict what happens in the future\n",
    "    - often the features best suited to predict a timeseries are previous values of the same timeseries\n",
    "- A note on smoothness and auto-correlation\n",
    "    - a common question to ask of a timeseries: how smooth is the data?\n",
    "    - or, how correlated is a timepoint with its neighboring timepoints (called **autocorrelation**)\n",
    "    - the amount of auto-correlation in data will impact your models\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating time-shifted features\n",
    "\n",
    "#In machine learning for time series, it's common to use information\n",
    "#about previous time points to predict a subsequent time point.\n",
    "\n",
    "#In this exercise, you'll \"shift\" your raw data and visualize the\n",
    "#results. You'll use the percent change time series that you\n",
    "#calculated in the previous chapter, this time with a very short\n",
    "#window. A short window is important because, in a real-world\n",
    "#scenario, you want to predict the day-to-day fluctuations of a\n",
    "#time series, not its change over a longer window of time.\n",
    "\n",
    "# These are the \"time lags\"\n",
    "#shifts = np.arange(1, 11).astype(int)\n",
    "\n",
    "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
    "#shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
    "\n",
    "# Convert into a DataFrame for subsequent use\n",
    "#prices_perc_shifted = pd.DataFrame(shifted_data)\n",
    "\n",
    "# Plot the first 100 samples of each\n",
    "#ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
    "#prices_perc.iloc[:100].plot(color='r', lw=2)\n",
    "#ax.legend(loc='best')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.28.svg](_images/15.28.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Special case: Auto-regressive models\n",
    "\n",
    "#Now that you've created time-shifted versions of a single time\n",
    "#series, you can fit an auto-regressive model. This is a regression\n",
    "#model where the input features are time-shifted versions of the\n",
    "#output time series data. You are using previous values of a\n",
    "#timeseries to predict current values of the same timeseries\n",
    "#(thus, it is auto-regressive).\n",
    "\n",
    "#By investigating the coefficients of this model, you can explore\n",
    "#any repetitive patterns that exist in a timeseries, and get an idea\n",
    "#for how far in the past a data point is predictive of the future.\n",
    "\n",
    "# Replace missing values with the median for each column\n",
    "#X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
    "#y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
    "\n",
    "# Fit the model\n",
    "#model = Ridge()\n",
    "#model.fit(X, y)\n",
    "\n",
    "#################################################\n",
    "#You've filled in the missing values with the median so that it\n",
    "#behaves well with scikit-learn. Now let's take a look at what your\n",
    "#model found."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualize regression coefficients\n",
    "\n",
    "#Now that you've fit the model, let's visualize its coefficients.\n",
    "#This is an important part of machine learning because it gives you\n",
    "#an idea for how the different features of a model affect the outcome.\n",
    "\n",
    "#The shifted time series DataFrame (prices_perc_shifted) and the\n",
    "#regression model (model) are available in your workspace.\n",
    "\n",
    "#In this exercise, you will create a function that, given a set of\n",
    "#coefficients and feature names, visualizes the coefficient values.\n",
    "\n",
    "#def visualize_coefficients(coefs, names, ax):\n",
    "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
    "#    ax.bar(names, coefs)\n",
    "#    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
    "\n",
    "    # Set formatting so it looks nice\n",
    "#    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "#    return ax\n",
    "\n",
    "# Visualize the output data up to \"2011-01\"\n",
    "#fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "#y.loc[:'2011-01'].plot(ax=axs[0])\n",
    "\n",
    "# Run the function to visualize model's coefficients\n",
    "#visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.29.svg](_images/15.29.svg)\n",
    "When you use time-lagged features on the raw data, you see that the\n",
    "highest coefficient by far is the first one. This means that the N-1th\n",
    "time point is useful in predicting the Nth timepoint, but no other points\n",
    "are useful."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Auto-regression with a smoother time series\n",
    "\n",
    "#Now, let's re-run the same procedure using a smoother signal.\n",
    "#You'll use the same percent change algorithm as before, but this\n",
    "#time use a much larger window (40 instead of 20). As the window\n",
    "#grows, the difference between neighboring timepoints gets smaller,\n",
    "#resulting in a smoother signal. What do you think this will do to\n",
    "#the auto-regressive model?\n",
    "\n",
    "#prices_perc_shifted and model (updated to use a window of 40) are\n",
    "#available in your workspace.\n",
    "\n",
    "# Visualize the output data up to \"2011-01\"\n",
    "#fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "#y.loc[:'2011-01'].plot(ax=axs[0])\n",
    "\n",
    "# Run the function to visualize model's coefficients\n",
    "#visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.30.svg](_images/15.30.svg)\n",
    "As you can see here, by transforming your data with a larger window,\n",
    "you've also changed the relationship between each timepoint and the\n",
    "ones that come just before it. This model's coefficients gradually\n",
    "go down to zero, which means that the signal itself is smoother over\n",
    "time. Be careful when you see something like this, as it means your\n",
    "data is not i.i.d. (independent and identically distributed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cross-validating time series data**\n",
    "___\n",
    "- K-fold cross-validation is usually used in timeseries data\n",
    "- shuffling data in cross validation only works for data that is i.i.d.\n",
    "    - do not use when making predictions with a timeseries\n",
    "- using the time series CV iterator\n",
    "    - always use data from the **past** *(training data)* to predict the **future** *(test data)*\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Cross-validation with shuffling\n",
    "\n",
    "#As you'll recall, cross-validation is the process of splitting your\n",
    "#data into training and test sets multiple times. Each time you do\n",
    "#this, you choose a different training and test set. In this exercise,\n",
    "#you'll perform a traditional ShuffleSplit cross-validation on the\n",
    "#company value data from earlier. Later we'll cover what changes need\n",
    "#to be made for time series data. The data we'll use is the same\n",
    "#historical price data for several large companies.\n",
    "\n",
    "#An instance of the Linear regression object (model) is available\n",
    "#in your workspace along with the function r2_score() for scoring.\n",
    "#Also, the data is stored in arrays X and y. We've also provided a\n",
    "#helper function (visualize_predictions()) to help visualize the\n",
    "#results.\n",
    "\n",
    "# Import ShuffleSplit and create the cross-validation object\n",
    "#from sklearn.model_selection import ShuffleSplit\n",
    "#cv = ShuffleSplit(n_splits=10, random_state=1)\n",
    "\n",
    "# Iterate through CV splits\n",
    "#results = []\n",
    "#for tr, tt in cv.split(X, y):\n",
    "    # Fit the model on training data\n",
    "#    model.fit(X[tr], y[tr])\n",
    "\n",
    "    # Generate predictions on the test data, score the predictions, and collect\n",
    "#    prediction = model.predict(X[tt])\n",
    "#    score = r2_score(y[tt], prediction)\n",
    "#    results.append((prediction, score, tt))\n",
    "\n",
    "# Custom function to quickly visualize predictions\n",
    "#visualize_predictions(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.31.svg](_images/15.31.svg)\n",
    "You've correctly constructed and fit the model. If you look at the\n",
    "plot above, see that the order of datapoints in the test set is\n",
    "scrambled. Let's see how it looks when we shuffle the data in blocks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Cross-validation without shuffling\n",
    "#Now, re-run your model fit using block cross-validation (without\n",
    "#shuffling all datapoints). In this case, neighboring time-points\n",
    "#will be kept close to one another. How do you think the model\n",
    "#predictions will look in each cross-validation loop?\n",
    "\n",
    "#An instance of the Linear regression model object is available in\n",
    "#your workspace. Also, the arrays X and y (training data) are\n",
    "#available too.\n",
    "\n",
    "# Create KFold cross-validation object\n",
    "#from sklearn.model_selection import KFold\n",
    "#cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
    "\n",
    "# Iterate through CV splits\n",
    "#results = []\n",
    "#for tr, tt in cv.split(X, y):\n",
    "    # Fit the model on training data\n",
    "#    model.fit(X[tr], y[tr])\n",
    "\n",
    "    # Generate predictions on the test data and collect\n",
    "#    prediction = model.predict(X[tt])\n",
    "#    results.append((prediction, tt))\n",
    "\n",
    "# Custom function to quickly visualize predictions\n",
    "#visualize_predictions(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.32.svg](_images/15.32.svg)\n",
    "This time, the predictions generated within each CV loop look\n",
    "'smoother' than they were before - they look more like a real time\n",
    "series because you didn't shuffle the data. This is a good sanity\n",
    "check to make sure your CV splits are correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Time-based cross-validation\n",
    "#Finally, let's visualize the behavior of the time series\n",
    "#cross-validation iterator in scikit-learn. Use this object to\n",
    "#iterate through your data one last time, visualizing the training\n",
    "#data used to fit the model on each iteration.\n",
    "\n",
    "#An instance of the Linear regression model object is available in\n",
    "#your workpsace. Also, the arrays X and y (training data) are\n",
    "#available too.\n",
    "\n",
    "# Import TimeSeriesSplit\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create time-series cross-validation object\n",
    "#cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# Iterate through CV splits\n",
    "#fig, ax = plt.subplots()\n",
    "#for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Plot the training data on each iteration, to see the behavior of the CV\n",
    "#    ax.plot(tr, ii + y[tr])\n",
    "\n",
    "#ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.33.svg](_images/15.33.svg)\n",
    "Note that the size of the training set grew each time when you used\n",
    "the time series cross-validation object. This way, the time points\n",
    "you predict are always after the timepoints we train on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Stationarity and stability**\n",
    "___\n",
    "- stationarity\n",
    "    - stationary timeseries do not change their statistical properties over time\n",
    "        -e.g., mean, standard deviation, trends\n",
    "    - most timeseries are non-stationary to some extent\n",
    "- stability\n",
    "    - non-stationary data results in variability in our model\n",
    "    - the statistical properties the model finds may change with the data\n",
    "    - we will be less certain about the correct values of model parameters\n",
    "- cross-validation to quantify parameter stability\n",
    "    - calculate model parameters on each iteration\n",
    "    - assess parameter stability across all CV splits\n",
    "- bootstrapping the mean\n",
    "    - a common way to assess variability\n",
    "    - take a random sample of data **with replacement**\n",
    "    - calculate the mean of the sample\n",
    "    - repeat this process thousands of times\n",
    "    - calculate the percentiles of the result (usually 2.5, 97.5)\n",
    "    - the result is a *95% confidence interval* of the mean of each coefficient\n",
    "- assessing model performance stability\n",
    "    - if using TimeSeriesSplit, we can *plot* the model's score over time\n",
    "    - this is useful in finding certain regions of time that hurt the score\n",
    "    - also useful to find non-stationary signals\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bootstrapping a confidence interval\n",
    "\n",
    "#A useful tool for assessing the variability of some data is the\n",
    "#bootstrap. In this exercise, you'll write your own bootstrapping\n",
    "#function that can be used to return a bootstrapped confidence\n",
    "#interval.\n",
    "\n",
    "#This function takes three parameters: a 2-D array of numbers (data),\n",
    "#a list of percentiles to calculate (percentiles), and the number of\n",
    "#bootstrap iterations to use (n_boots). It uses the resample function\n",
    "#to generate a bootstrap sample, and then repeats this many times to\n",
    "#calculate the confidence interval.\n",
    "\n",
    "#from sklearn.utils import resample\n",
    "\n",
    "#def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
    "#    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
    "    # Create empty array to fill the results\n",
    "#    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
    "#    for ii in range(n_boots):\n",
    "        # Generate random indices for data *with* replacement, then take the sample mean\n",
    "#        random_sample = resample(data)\n",
    "#        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
    "\n",
    "    # Compute the percentiles of choice for the bootstrapped means\n",
    "#    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
    "#    return percentiles\n",
    "\n",
    "#################################################\n",
    "#You can use this function to assess the variability of your model\n",
    "#coefficients."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Calculating variability in model coefficients\n",
    "\n",
    "#In this lesson, you'll re-run the cross-validation routine used\n",
    "#before, but this time paying attention to the model's stability\n",
    "#over time. You'll investigate the coefficients of the model, as\n",
    "#well as the uncertainty in its predictions.\n",
    "\n",
    "#Begin by assessing the stability (or uncertainty) of a model's\n",
    "#coefficients across multiple CV splits. Remember, the coefficients\n",
    "#are a reflection of the pattern that your model has found in the\n",
    "#data.\n",
    "\n",
    "#An instance of the Linear regression object (model) is available\n",
    "#in your workspace. Also, the arrays X and y (the data) are\n",
    "#available too.\n",
    "\n",
    "# Iterate through CV splits\n",
    "#n_splits = 100\n",
    "#cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Create empty array to collect coefficients\n",
    "#coefficients = np.zeros([n_splits, X.shape[1]])\n",
    "\n",
    "#for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Fit the model on training data and collect the coefficients\n",
    "#    model.fit(X[tr], y[tr])\n",
    "#    coefficients[ii] = model.coef_\n",
    "\n",
    "# Calculate a confidence interval around each coefficient\n",
    "#bootstrapped_interval = bootstrap_interval(coefficients)\n",
    "\n",
    "# Plot it\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
    "#ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
    "#ax.set(title='95% confidence interval for model coefficients')\n",
    "#plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.34.svg](_images/15.34.svg)\n",
    "You've calculated the variability around each coefficient, which\n",
    "helps assess which coefficients are more stable over time!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing model score variability over time\n",
    "#Now that you've assessed the variability of each coefficient, let's\n",
    "#do the same for the performance (scores) of the model. Recall that\n",
    "#the TimeSeriesSplit object will use successively-later indices for\n",
    "#each test set. This means that you can treat the scores of your\n",
    "#validation as a time series. You can visualize this over time in\n",
    "#order to see how the model's performance changes over time.\n",
    "\n",
    "#An instance of the Linear regression model object is stored in\n",
    "#model, a cross-validation object in cv, and data in X and y.\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Generate scores for each split to see how the model performs over time\n",
    "#scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "\n",
    "# Convert to a Pandas Series object\n",
    "#scores_series = pd.Series(scores, index=times_scores, name='score')\n",
    "\n",
    "# Bootstrap a rolling confidence interval for the mean score\n",
    "#scores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5))\n",
    "#scores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5))\n",
    "\n",
    "# Plot the results\n",
    "#fig, ax = plt.subplots()\n",
    "#scores_lo.plot(ax=ax, label=\"Lower confidence interval\")\n",
    "#scores_hi.plot(ax=ax, label=\"Upper confidence interval\")\n",
    "#ax.legend()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.35.svg](_images/15.35.svg)\n",
    "You plotted a rolling confidence interval for scores over time. This\n",
    "is useful in seeing when your model predictions are correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Accounting for non-stationarity\n",
    "\n",
    "#In this exercise, you will again visualize the variations in model\n",
    "#scores, but now for data that changes its statistics over time.\n",
    "\n",
    "#An instance of the Linear regression model object is stored in model,\n",
    "#a cross-validation object in cv, and the data in X and y.\n",
    "\n",
    "# Pre-initialize window sizes\n",
    "#window_sizes = [25, 50, 75, 100]\n",
    "\n",
    "# Create an empty DataFrame to collect the stores\n",
    "#all_scores = pd.DataFrame(index=times_scores)\n",
    "\n",
    "# Generate scores for each split to see how the model performs over time\n",
    "#for window in window_sizes:\n",
    "    # Create cross-validation object using a limited lookback window\n",
    "#    cv = TimeSeriesSplit(n_splits=100, max_train_size=window)\n",
    "\n",
    "    # Calculate scores across all CV splits and collect them in a DataFrame\n",
    "#    this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "#    all_scores['Length {}'.format(window)] = this_scores\n",
    "\n",
    "# Visualize the scores\n",
    "#ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm)\n",
    "#ax.set(title='Scores for multiple windows', ylabel='Correlation (r)')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/15.36.svg](_images/15.36.svg)\n",
    "notice how in some stretches of time, longer windows perform worse\n",
    "than shorter ones. This is because the statistics in the data have\n",
    "changed, and the longer window is now using outdated information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Wrap-up**\n",
    "___\n",
    "- Timeseries and machine learning\n",
    "    - the many applications of time series + machine learning\n",
    "    - always visualize the data first\n",
    "    - the scikit-learn API standardizes this process\n",
    "- Feature extraction and classification\n",
    "    - summary statistics for time series classification\n",
    "    - combining multiple features into a single input matrix\n",
    "    - feature extraction for time series data\n",
    "- Model fitting and improving data quality\n",
    "    - time series features for regression\n",
    "    - generating predictions over time\n",
    "    - cleaning and improving time series data\n",
    "- Validating and assessing our model performance\n",
    "    - cross-validation with time series data (don't shuffle the data!)\n",
    "    - time series stationarity\n",
    "    - assessing model coefficient and score stability\n",
    "- Advanced concepts in time series\n",
    "    - advanced window functions\n",
    "    - signal processing and filtering details\n",
    "    - spectral analysis\n",
    "- Advanced machine learning\n",
    "    - advanced time series feature extraction\n",
    "        - e.g., tsfresh\n",
    "    - more complex model architectures for regression and classification\n",
    "    - production-ready pipelines for time series analysis\n",
    "- Ways to practice\n",
    "    - Kaggle\n",
    "    - Quantopian\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}